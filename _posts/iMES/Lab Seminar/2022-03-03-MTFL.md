---
title: "Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing"
author: Joe2357
categories: [1. iMES, Lab Seminar]
tags: [Lab Seminar]
math: true
---

> 2022 / 4 / 7 iMES 세미나

## Abstract

- 연합학습 : private한 user data를 전송하지 않으면서 DNN을 돌리는 새로운 방법
- 이전의 연구들 : non-IID의 user data가 FL 알고리즘 수렴 속도에 해를 끼친다고 평가
  - **non-Independent and Identically Distributed** ( non-IID ) user data : client들이 가지고 있는 각 data들은 서로 독립 + 동일한 확률분포를 가지고 있지 않다
- 대부분의 FL 연구 : global model의 정확도를 측정함
  - 하지만 여러 상황에서, 개인 user model의 정확도 ( UA ) 를 향상하는 것이 주요 목표가 되어야할 것
- <u>Multi-Task FL 알고리즘</u> 제안 : 연합 DNN 안에 **non-federated Batch-Normalization layer**를 도입
  - 효과 : UA와 수렴 속도 향상 ( user가 자신의 data를 이용하여 개인화된 model을 훈련시킬 수 있도록 )
  - FedAvg와 호환 / FedAvg-Adam이 수렴 속도를 더 빠르게 함
  - UA에 도달하기 위한 round 수를 최대 5배까지 줄일 수 있음



## Introduction

- Multi-Access Edge Computing ( MEC ) : cloud service를 network edge로 옮김 -> latency 줄임, 실시간 처리 가능하도록
  - 각 device들의 data 수집량이 급격히 증가되자, 이 data를 source측에 가깝게 저장하고 처리하려함
- Deep Neural Networks ( DNN ) : 넓은 범위의 잠재적인 app / 배포 용이성 / 최첨단 성능 등으로 인해 인기가 높아지고 있음
  - 단점 : supervised learning이므로 <u>계산 비용이 많이 듬 / 엄청난 양의 교육 data가 필요</u>
- MEC의 DNN 사용 : device에서 data를 수집 -> cloud에서 training 수행 -> edge로 모델 배포
  - data 프라이버시 문제 존재 ( 민감한 data의 업로드를 꺼리기 시작 )
  - **그렇다면 모델들을 어떻게 훈련시킬 수 있을까?**
- Federated Learning : client들의 data는 공개되지 않으면서 머신러닝 모델을 training
  - FedAvg : 한 round마다 client들은 자신만의 model을 학습
    - 이후 각 모델을 중앙서버로 올려 최종 통합모델을 학습
  - 사용자에게 민감한 data가 필요한 경우 ( health care, 다음 단어 예측 등 ) 효율적인 방법
  - 도전과제 존재 ( challenge )
    - user들이 IID data를 가지고 있지 않을 수 있음
      - noise data가 될 가능성, 결국 FL model을 방해하는 요소
    - FL 연구가 현재는 중앙모델의 정확도를 높이는 데에만 집중되어있음 ( **개인 모델의 정확도를 높이는 것이 주요 목표가 되어야할 것** )
    - global model의 정확도가 개인의 model보다 성능이 안좋을 수도 있음 ( non-iid 때문에 )
- Multi-Task FL 제안 : client가 local model 정확도를 향상 + client privacy를 향상 + **개인화된 DNN 훈련 가능**
  - 다른 개인화 FL 알고리즘에 비해 낮은 storage / computing 비용
  - FL client들의 data는 non-IID임 -> client들은 서로 다른 작업에 대한 local train을 통해 model 최적화 시도
    - MTFL : **Batch 표준화 계층** ( BN layer ) 추가 // 각 client에게는 비공개로 유지
      - 이점
        - client들의 local data를 개인화할 수 있음 // data의 개인정보를 보호할 수 있음
        - storage 비용 이점이 있음 // 전체 DNN parameter 중 BN 매개변수만을 사용함

      - iterate FL framework에 **Personalisation** 추가 ( 개인화 추가 )
        - FedAvg 등과 같은 대부분의 알고리즘은 vanilla Stochastic Gradient Descent ( SGD ) 방식을 사용하지만, 운동량 기반 최적화 전략 ( ex. Adam ) 이 FL 수렴속도를 향상시킬 수 있다
        - FedAvg-Adam을 사용하면 통신 round 속도에 상당한 속도 향상을 보여줌

- Contribution
  - 기존 반복 FL 알고리즘에 Multi-Task를 추가한 MTFL 제안
    - 개인 data를 이용한 DNN 모델 훈련을 가능하도록 조절
    - private DN 계층 추가하여 privacy 이점 높임

  - 새로운 알고리즘 평가 기준인 User model Accuracy ( UA ) 제안
    - 표준 global model 정확도보다 common object를 더 잘 반영

  - 개인 BN 계층이 MTFL 모델 활성화에 미치는 영향 분석 ( 영향 원인 / 통찰력 제공 )
    - train된 parameter 또는 BN 계층의 통계를 비공개로 유지하는 경우 MTFL의 훈련 / 테스트 성능을 분석

  - 광범위한 시뮬레이션 수행 ( 다른 FL 알고리즘에 비해 3~5배 높은 UA를 만들 수 있었음 )
  - MEC 비슷한 testbed에서 수행평가
    - 구성품 : 라즈베리파이 client들 + 하나의 FL서버
    - FedAvg-Adam의 overhead가 있더라도 UA, 수렴 속도 등에 비해서는 미미하다는 것을 보여줌



## Multi-Task Federated Learning ( MTFL )

- 알고리즘은 client-server 기반을 기본적으로 따름
  - 그러나 round는 server에 의해 시작되어짐
  - round 작동원리
    - Step 1 : server가 database에서 client들을 선택 ( all / subset ) 하여 `Work Request` 보냄
    - Step 2 : user preference에 의해서 참여할 user들은 `Accept` 보냄
    - Step 3 : server -> client로 global model 전송
      - client들은 개인 patch를 이용하여 global model copy를 보강
    - Step 4 : client들은 자체 data를 이용하여 다른 model을 학습시킴
      - 새로운 model의 patch layer를 local에 저장 / non-private model parameter를 server에 upload
    - Step 5 : server는 client들이 model을 upload할 때까지 대기
      - sever 환경설정에 따라 최대 제한 시간을 두거나 client들을 계속 기다리게 할 수 있음
    - Step 6 : 새로운 round 시작 전에 모든 model을 집계하여 새로운 global model 생성
- MTFL : 실제 모델 learning을 수행하는 client device로 계산을 offload
  - FedAvg, 다른 개인화된 FL보다 **user의 data privacy를 강력히 보존**
  - user data, local model의 주요 부분 또한 upload되지 않음
  - patch layer를 사용 -> user의 non-IID dataset에서 model 성능을 향상, MTFL 개인화

### 1. User Model Accuracy and MTFL

- 많은 FL 작업들은 FL 성능을 측정하기 위해 중앙 IID testset을 사용함

  - 바람직할 수도 있고, **바람직하지 않을 수도 있다** ( 어떠한 model을 만들 것이냐에 따라 다를 것 )
    - FL의 궁극적인 목적 : <u>"개별 user device에서 성능이 좋은 model을 만드는 것"</u>
  - 예시 : Google의 GBoard ( 입력된 단어를 보고 다음 단어를 예측하는 keyboard )
    - 일반적으로 user가 non-IID data를 가지고 있지 않아, 특정 user에게는 좋은 성능을, 특정 user는 안좋은 성능을 보였음

- 새로운 FL의 성능 측정 단위가 필요 : User model Accuracy  (UA)

  - local test set을 사용하는 client의 정확도를 나타냄
  - 각 client들의 testset은 train data와 유사한 분포로 추출해야함
  - 다른 측정 방식으로 변경할 수 있을 것 ( error, recall 등 )

- FL에서 user data는 대체로 non-IID임 -> user는 달라도 관련 학습 작업이 있다고 간주 가능

  - FL 체계가 전체 model 정확도는 높일 수 있으나, UA에서는 성능이 좋지 않을 것 ( contribute가 적은 user들의 고려가 많이 안될 수 있음 )

- MTFL 알고리즘 제안 ( 평균 UA의 향상 목적 )

  - small per-task에서, DNN에 <u>patch layer</u>를 추가하여 MTL 시나리오에서 성능이 향상함을 관찰

- FL에서의 목적 : 아래의 objective 함수를 최소화하는 것
  $$
  \begin{equation*} F_{\mathrm{FL}} = \sum _{k=1}^K \frac{n_k}{n} \ell _k(\Omega) \tag{1} \end{equation*}
  $$

  |  Symbol  |                Meaning                |
  | :------: | :-----------------------------------: |
  |   $K$    |               client 수               |
  |  $n_k$   |  client k가 가지고 있는 sample 개수   |
  |   $n$    | 모든 client가 가지고 있는 sample 개수 |
  |  $l_k$   |       client k의 loss function        |
  | $\Omega$ |      global model parameter set       |

  - MTFL에서의 object 함수는 unique client patch를 포함시켜야함
    $$
    \begin{equation*} F_{\mathrm{MTFL}} = \sum _{k=1}^K \frac{n_k}{n} \ \ell _k(\mathcal {M}_k) \tag{2} \end{equation*}
    $$

    $$
    \begin{equation*} \mathcal {M}_k = (\Omega _1 \cdots \Omega _{i_1}, P_{k_1}, \Omega _{i_1 + 1} \cdots \Omega _{i_m}, P_{k_m}, \Omega _{i_m + 1} \cdots \Omega _j) \tag{3} \end{equation*}
    $$

    |           Symbol           |                           Meaning                            |
    | :------------------------: | :----------------------------------------------------------: |
    |      $\mathcal {M}_k$      |                   client k의 patched model                   |
    | $\Omega_1 \cdots \Omega_j$ | Fedarated model parameter <br>j : federated layer의 전체 수  |
    |  $P_{k_1} \cdots P_{k_m}$  | patch parameter <br>m : local patch의 전체 수 <br>\{ i \} : patch parameter의 index set |

- MTFL : MTL을 FL에 통합하기 위한 알고리즘

  - MTFL 내에서는 다른 최적화 전략을 사용할 수도 있음
  - 어떤 최적화 전략을 쓰더라도, target UA에 도달하기 위한 round 수는 크게 줄일 수 있음

- MTFL 알고리즘

  ```Algorithm
  Initialise global model Ω and global optimiser values V
  while termination criteria not met do
      Select round clients, Sr⊂S, |Sr|=C⋅|S|
      for each client sk∈Sr in parallel do
          Download global parameters Mk←Ω
          Download optimiser values Vk←V
          for i∈ patchIdxs do ⊳ Apply local patches
              Mk,i←Pk,i, Vk,i←Wk,i
          end for
          for batch b drawn from local data Dk do
              Mk,Vk←LocalUpdate(Mk,Vk,b)
          end for
          for i∈ patchIdxs do ⊳ Save local patches
              Pk,i←Mk,i,Wk,i←Vk,i
          end for
          for each i∉ patchIdxs do
              Upload Mk,i,Vk,i to server
          end for
      end for
      for i∉ nonPatchIndexes do
          Ωi←GlobalModelUpdate(Ωi,{Mk,i}k∈Sr)
          Vi←GlobalOptimUpdate(Vi,{Vk,i}k∈Sr)
      end for
  end while
  ```

  - Line 2 : 주어진 종료 기준 ( ex. UA )을 만족하기 전까지 round를 반복
  - Line 3 : client들을 subset으로부터 선택
  - Line 5 ~ 6 : model 매개변수, global optimiser를 download받음
  - Line 7 ~ 9 : global model과 optimiser 복사본을 개인 patch layer를 이용하여 update
    - `patchIdxs` : DNN에서의 patch layer 배치 index를 포함
  - Line 10 : 개인화된 global model 복사본을 local data를 이용하여 학습
    - 어떤 전략을 사용할 것인지는 결정해야함
  - Line 11 ~ 13 : update된 local patch는 저장되어짐
  - Line 14 ~ 16 : non-patch layer와 optimiser는 server로 upload
  - Line 18 ~ 20 : server는 새로운 global model과 optimiser를 생성
  - MTFL의 round당 계산 복잡도는 참여 client 수인 $\|S_r\|$로 확장 가능
    - 각 client가 수행하는 계산은 병렬로 행해지기 때문에 client 수와는 무관 // 확장성이 좋음

- Batch Normalisation layer가 중앙집중식 설정에서 MTL의 model patch로 작동할 수 있음

  - BN layer가 매개변수의 수 측면에서 매우 lightweight함 -> **MTFL에서 patch로 잘 작동함을 보임**
    $$
    \begin{align*} &\hat{x}_i = \frac{z_i - \mathbb {E}(z_i)}{\sqrt{\mathrm{Var}(z_i) + \epsilon }} \\ &\mathrm{BN}(\hat{x}_i) = \gamma _i \hat{x}_i + \beta _i, \tag{4} \end{align*}
    $$

    |        Symbol         |                Meaning                |
    | :-------------------: | :-----------------------------------: |
    |  $\mathbb {E}(z_i)$   | mini batch에서 neuron의 활성화의 평균 |
    |  $\mathrm{Var}(z_i)$  | mini batch에서 neuron의 활성화의 분산 |
    | $\gamma _i, \beta _i$ |            학습된 매개변수            |

    - BN layer는 추론 시간에서 사용하기 위해 weighted moving average ( 가중 이동 평균 $μ_i, σ^2_i$ ) 를 tracking

- MTFL에서 개인화를 위해서 BN layer를 사용하는 것임

  - 우수한 개인화 성능을 보일 수 있었음
  - BN 매개변수 저장 cost가 매우 적었음 ( 전체 model의 1% 미만 )
  - 모든 model layer는 원칙적으로 MTFL에서 비공개로 유지될 수 있지만, <u>매개 변수의 수와 model 수렴능력 사이에는 절충</u>이 존재함

### 2. Effect of BN Patches on Inference

- UA는 반복 FL 작업에서 집합 단계 이후 살짝씩 drop됨

  - model이 local training에 맞춰져 있다가 가중치가 갑자기 연합 가중치로 대체되기 때문 ( 전체 모델이 사전집계 모델보다 성능이 안좋을 수 있기 때문 )

- 조밀한 층으로 구성된 단순한 DNN과 BN

  - weights와 biases ( $W_0$, $b_0$ )를 적용하여 client test set X에 걸친 first-layer 뉴런 활성화 vector는 정규 분포로 모델링될 수 있음 ( BN은 이에 의존함 )
    $$
    \begin{align*} &z_i \triangleq [W_0 X + b_0 ]_i \\ z_i &\sim N(\mathbb {E}[z_i], Var[z_i]). \tag{5} \end{align*}
    $$

  - local training 동안 client model은 local dataset에 적용되어짐

    - inference에 사용되는 BN-layer 통계는 계층 활성화에서 update되어짐
    - local training의 결과 ( $μ_i ≈ \mathbb {E}[z_i], σ^2_i ≈ Var[z_i]$ ) 를 가정하면 아래와 같아짐

    $$
    \begin{align*} &\hat{x}_i \triangleq \frac{z_i - \mu _i}{\sigma _i} \\ &\hat{x}_i \sim N(0, 1) \\ \mathrm{BN}&(\hat{x}_i) \sim N(\beta _i, \gamma _i^2), \tag{6} \end{align*}
    $$

    - $\beta_i, \gamma^2_i$ : 학습된 BN parameter

  - client가 FL에 참여하는 경우, model parameter인 $W_0, b_0$은 연합되어진 값 $\bar{W_0}, \bar{b_0}$으로 download 이후 update되어짐

    - first-layer의 활성화식은 아래와 같아짐

    $$
    \begin{align*} &\overline{z}_i \triangleq [\overline{W}_0 X + \overline{b}_0 ]_i \\ \overline{z}_i &\sim N(\mathbb {E}[\overline{z}_i], \mathrm{Var}[\overline{z}_i]). \tag{7} \end{align*}
    $$

  - aggregation 전후 활성화 간 평균 / 분산의 차이를 정의
    $$
    \begin{align*} \Delta{μ_i} = \mathbb {E}[\bar{z_i}] − \mathbb {E}[z_i], \Delta{σ^2_i} = Var[\bar{z_i}] − Var[z_i] \end{align*}
    $$

  - 위 결과로 얻을 수 있는 MTFL의 BN-patch layer의 output
    $$
    \begin{align*} &\hat{\bar{x}}_i \sim N \left(\frac{\Delta \mu _i}{\sigma _i}, 1 + \frac{\Delta \sigma _i^2}{\sigma _i^2} \right) \\ \mathrm{BN}(\hat{\bar{x}}_i) &\sim N \left(\gamma \frac{\Delta \mu _i}{\sigma _i} + \beta _i, \gamma _i^2 \left(1 + \frac{\Delta \sigma _i^2}{\sigma _i^2} \right) \right). \tag{8} \end{align*}
    $$

  - BN layer가 patch layer가 아닌경우 ( client가 연합된 값을 사용하면서 FL에 참여하는 경우 ) BN layer의 output
    $$
    \begin{align*} &\hat{\bar{x}}_i \sim N \left(\frac{\mu _i+\Delta \mu _i-\bar{\mu }_i}{\bar{\sigma }_i}, \frac{\sigma _i^2+\Delta \sigma _i^2}{\bar{\sigma }_i^2} \right) \\ \overline{\mathrm{BN}}(\hat{\bar{x}}_i) &\sim N \left(\bar{\gamma } \frac{\mu _i+\Delta \mu _i-\bar{\mu }_i}{\bar{\sigma }_i} + \bar{\beta }_i, \bar{\gamma }_i^2 \frac{\sigma _i^2+\Delta \sigma _i^2}{\bar{\sigma }_i^2} \right). \tag{9} \end{align*}
    $$

  - BN-layer를 MTFL에 이용하는 것이 FL의 non-patch BN layer와 비교했을 때 **뉴런 활성화가 응집단계 이전과 더 가깝도록 제한할 수 있다**

    - MTFL을 사용하는 집계의 평균 / 분산의 차이는 FL을 사용할 때보다 적음을 보였다

    $$
    \begin{align*} &\left| \gamma \frac{\Delta \mu _i}{\sigma _i} \right| < \left| \beta _i - \bar{\gamma } \frac{\mu _i+\Delta \mu _i-\bar{\mu }_i}{\bar{\sigma }_i} - \bar{\beta }_i \right| \\ &\quad \left| \gamma _i^2\frac{\Delta \sigma _i^2}{\sigma _i^2} \right| < \left| \gamma _i^2 - \bar{\gamma }_i^2 \frac{\sigma _i^2+\Delta \sigma _i^2}{\bar{\sigma }_i^2} \right|. \tag{10} \end{align*}
    $$

    - 위 부등식이 성립한다고 가정한다면, FL BN layer가 아닌 BN patch를 사용하면 **first-layer 이후에 network를 통해 전파되는 값이 어떻게 사전에 집약한 값과 가까운지** 알 수 있음
      - network 전체에 BN patch가 추가되면 중간 DNN의 값은 정기적으로 사전집약된 값에 <u>제한되어</u> 결과적으로 사전 집약된 값에 가까운 network 출력이 됨
    
  - $\Delta \mu _i$와 $\Delta \sigma _i^2$가 크다면, BN-patch layer 이후의 neuron의 output distribution $\mathrm{BN}(\hat{\bar{x}}_i)$은 $\mathrm{BN}(\hat{x}_i)$와는 상당히 다른 값을 가질 것
  
    - BN patch layer는 federated BN layer에 비해 거의 이득이 없을 것
    - 방정식 10번에서 왼쪽이 오른쪽보다 훨씬 작지 않기 때문
      - 사전 / 사후 집계된 model parameter의 큰 차이는 gradient가 크고 client model이 더 많이 분기되어지는 **초기 traning 단계에서 나타남**
      - MTFL : 훈련 초기단계에서의 이점은 적음 / gradient의 크기가 감소하면 그만큼 이점이 증가하는 경향

### 3. Federated Optimisation Within MTFL

- MTFL : 각 client에 대해 개인 patch layer를 적용, federated layer와 함께 training

  - 각 round가 끝나면 server는 client에서 upload된 FL layer를 집계 / 새로운 global model 생성

  | 최적화 전략 | LocalUpdate | GlobalModelUpdate | GlobalOptimUpdate |
  | :---------: | :---------: | :---------------: | :---------------: |
  |   FedAvg    |     SGD     |      Average      |         -         |
  |   FedAdam   |     SGD     |       Adam        |         -         |
  | FedAvg-Adam |    Adam     |      Average      |      Average      |

## Experiments

- 모든 실험에 사용된 dataset, model 및 data 분할 체계에 대한 정보 제공
- MTFL이 목표 UA에 도달하기 위해 진행한 round 수에 미치는 영향을 분석 / 실험 제시
  - 비공개로 유지되는 경우 **어떤 BN 값이 최상의 성능을 내는지 조사**
- 다른 개인 BN이 training에 왜 다른 영향을 미치는지 조사
- MTFL과 다른 최첨단 개인화 FL 기술과 비교

### 1. Datasets and Models

- 2개의 image 분류 dataset인 MNIST, CIFAR10 사용
  - MNIST : 28 x 28 픽셀의 손으로 쓴 grayscale image
    - feature 개수 : 10개
    - dataset에 사용된 network : 2NN
      - 200개의 뉴런으로 구성된 하나의 Fully Connected layer
      - BN layer
      - 200개의 뉴런으로 구성된 2번째 FC layer
      - softmax 출력 layer
  - CIFAR10 : 32 x 32 픽셀의 객체 RGB image
    - feature 개수 : 10개
    - dataset에 사용된 network : CNN
      - 32개의 filter가 있는 하나의 3x3 conv layer, BN, ReLU, 2x2 max pooling
      - 2번째 64개의 filter가 있는 3x3 conv layer, BN, ReLU, 2x2 max pooling
      - 512개의 뉴런이 있는 ReLU FC layer
      - softmax 출력 layer
- 실험은 non-IID client에서 client의 수 $W$, client의 참여율 $C$, 최적화 전략을 다양하게 하며 진행
  - non-IID data를 만들기 위해 아래와 같은 방식 사용
    - training, testset을 label별로 ordering
    - 각각을 2W개의 파편으로 분할
    - 각 client들에게 2개씩 분배

### 2. Patch Layers in FL

- Setup
  - UA의 목표치에 도달하기 위해 필요한 round의 수를 비교
    - FL : model parameter가 비공개가 아님 -> patch가 없음
    - MTFL : 일부 model parameter는 비공개로 유지됨
  - local epoch $E = 1$로 고정하고 모든 시나리오에서 learning rate hyperparameter를 조정 -> 가장 적은 round를 가지도록 조절
    - FedAvg, FedAvg-Adam : 각 시나리오에서 1개의 hyperparameter만 조절하면 되었음
    - FedAdam : client, server의 학습 속도를 모두 training해야했음
  - noise data를 가진 client가 있는 경우 또한 조사했음
    - 무작위로 20%의 client가 0-mean Gaussian noise를 훈련 data에 추가했음
- Result
  - 모든 BN-layer의 값 ( $\mu, \sigma, \gamma, \beta$ ) 이 비공개로 유지될 때, FL보다 round 수가 적음을 보임
  - BN patch의 추적된 통계만 비공개로 하면 **오히려 성능이 손상이 있음**
    - 반대로 개인 훈련 가능한 parameter만 있는 MTFL은 **훨씬 더 적은 round를 보임**
  - MTFL은 non-identical한 user model이 UA의 분산에 기여하기 때문에, train 중에 UA의 분산을 증가시킴
    - 하지만 실제로 실험 결과 FL과 MTFL에서의 UA 분산 차이는 거의 없었음
  - MTFL이 noisy client가 아닌 client에게 미치는 영향을 완화시킬 수 있음
    - non-noisy한 client들이 noisy client로부터 "분리"되기 때문일 것
    - 모든 model parameter를 공유하는 것이 아니므로 noisy client의 참여로 인한 피해를 입은 global model을 수신하는 영향이 줄어들게 됨
  - FedAvg-Adam이 다른 최적화 전략과 비교하였을 때, 모든 것을 막론하고 가장 좋은 성능을 보였음

### 3. Training and Testing Results Using MTFL

- Setup
  - BN layer의 비공개 변수 ( $\mu, \sigma$ or $\gamma, \beta$ ) 가 다를때 결과가 왜 다르게 나오는지 조사
  - 하나의 시나리오에서의 UA를 훈련, 시험하는 방법에 대해 plotting 진행
  - 600회의 통신 round / 라운드마다 10단계의 local training 진행 -> 6000개의 전체 단계를 나타냄
- Results
  - MTFL과 FL에서, private $\mu, \sigma$가 있는 training curve는 둘이 같음
    - BN statistics는 test 시간에만 사용되고, 훈련에 영향을 미치지 않기 때문
    - 대신 test 정확도는 FedAvg보다 낮을 수 있음
      - BN 값의 불일치가 원인 ( $\gamma, \beta$는 평균을 내므로 다른 분포를 출력 -> 이러한 개인 통계가 추적되면 model의 기능을 손상시킴 )
    - 실질적으로, 모든 parameter를 비공개로 하는 것과 $\gamma, \beta$만 비공개로 하는것은 성능 차이가 크게 없음을 보임
  - $\gamma, \beta$를 비공개로 유지하면 훈련 정확도가 향상될 수 있는 비율이 큼
    - FedAvg가 client model에 대한 일종의 정규화로써 작동할 수 있음
    - client dataset이 작을 경우, 독립적으로 훈련된 model은 overfitting 되기 쉬움, training error가 빠르게 0에 가까워짐
      - 대신 일반화의 성능이 좋지 않음 ( local의 정확도에만 치중하게 되므로 )
    - 일부 model parameter를 비공개로 하는 것은 빠른 수렴, 정규화 사이의 균형을 맞추는 것

### 4. Personalised FL Comparison

- Setup
  - MTFL와 FL, 최첨단 개인화 알고리즘 ( Per-FedAvg, pFedMe ) 와의 비교
    - 각 알고리즘의 hyperparameter를 조정하여 200회 통신 round 내에서 최대 평균 UA를 달성
  - 개인화 알고리즘과의 비교를 위해, MTFL은 FedAvg + private( $\gamma, \beta$ )를 가진 알고리즘으로 사용하여 비교했음
  - MTFL, FL의 hyperparameter는 1개 // Per-FedAvg, pFedMe의 hyperparameter는 2개
- Result
  - MTFL이 다른 scheme들보다 더 좋은 UA 결과를 가져왔다
  - Per-FedAvg와 pFedMe는 MNIST에서, FL과 비교하였을 때, W = 200에서는 성능이 좋았지만 W = 400에서는 오히려 안좋아짐
  - CIFAR10에서는 모든 scheme이 FL보다는 성능이 좋았음
    - task 자체가 MNIST보다 어려운 것이 그 이유일 것
  - MTFL은 Per-FedAvg, pFedMe보다 성능이 좋았고, 튜닝해야할 hyperparameter도 하나 적음 // 계산비용이 저렴
    - \+ 일부 model parameter가 비공개이므로 개인정보 보호 효과도 있음

### 5. Testbed Results

- Setup
  - 현실적인 MEC 환경을 제작하기 위해 WiFi를 통해 server에 연결된 5개의 라즈베리파이 ( RPi ) 2B, 5개의 RPi 3B로 구성된 10개의 client 사용
    - 저전력 / 이기종 client set를 만들어 에뮬레이션
  - Tensorflow를 사용하여 local training 수행
  - server는 model test를 하지 않음 ( model 수신, 평균, 배포 역할만 함 )
  - round 시간을 평균내고, download / training / upload / aggregation 에서 사용한 시간을 백분율로 기록
- Result
  - Independent learning : client가 model을 download / upload할 이유가 없으므로 가장 짧은 시간이 걸림
  - FL : upload / download에서 시간을 오래 사용
  - MTFL : FedAvg를 이용하여, FedAvg-Adam이 전하는 가중치의 수가 증가했음
    - **round당 시간이 가장 길었음** ( model download / upload에서 소요되는 round 시간 비율이 높았음 )
    - 그러나, 통신 시간의 증가는 대부분 목표 UA에 도달하는데 필요한 round의 수가 훨씬 적으므로, 최종적으로는 좋은 성능을 보일 수 있음
  - 이 실험에서는 local training에서의 시간 비율이 너무 높음 ( RPi의 낮은 연산성능 + DNN model training의 높은 계산비용 )
    - 실제 상황에서는 client 컴퓨터의 연산능력, 사용된 model의 연산 cost, 통신조건 등에 의해 round 시간은 달라질 수 있음

## Conclusion

- global model에 private patch-layer를 도입하여 반복적인 FL을 기반으로 하는 **MTFL 알고리즘 제안**
  - private layer로 인해 user는 개인화된 model을 가질 수 있으며, UA가 크게 향상됨
- MTFL에서 BN layer를 patch로 사용, 분석 -> 이점에 대한 설명 제시
- MTFL : 특정 FL 최적화 전략을 요구하는 알고리즘 ( FedAvg-Adam을 제안 )
- MNIST, CIFAR10을 이용한 실험을 진행하여, FedAvg를 사용하면 <u>목표 UA 달성까지의 round 수를 최대 5배 줄임</u>을 보임
  - FedAvg-Adam을 사용하면 최대 3배 더 줄임을 보임
- model patch에서, 통계 ( $\mu, \sigma$ ) 보다 BN 훈련 가능 parameter ( $\gamma, \beta$ ) 를 사용하는 것이 **더 빠른 수렴속도를 제공**
- 다른 최첨단 알고리즘과 비교하였을 때, MTFL이 가장 높은 UA를 나타냄을 보임

